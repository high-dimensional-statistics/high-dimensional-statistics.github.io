---
layout:      post
title:       "Exercise 15.4: More Properties of Shannon Entropy"
sorting_tag: "1504"
tags:        [chapter 15]
comments:    true
---

## (a)

Use non-negativity of the mutual information:
\begin{align}
  0
  \leq
  I(X ; Y)
  =
  \E \bigl[
    \log \tfrac{p(X \given Y)}{p(X)}
  \bigr]
  =
  H(X)
  -
  H(X \given Y)
  \, .
\end{align}


## (b)

\begin{align}
  H(X, Y, Z)
  =
  \- \E \bigl\lbrace \log \bigl[ p(X) p(Y \given X) p(Z \given X, Y) \bigr] \bigr\rbrace
  =
  H(X) + H(Y \given X) + H(Z \given X, Y)
\end{align}


## (c)

Follows from (a) applied to the r.h.s. of (b).
